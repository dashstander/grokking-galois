{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grokking Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup\n",
    "(No need to read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_139467/3667787199.py:21: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"load_ext autoreload\")\n",
      "/tmp/ipykernel_139467/3667787199.py:22: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "  ipython.magic(\"autoreload 2\")\n"
     ]
    }
   ],
   "source": [
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "DEVELOPMENT_MODE = True\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git@new-demo\n",
    "    %pip install circuitsvis\n",
    "    \n",
    "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
    "    # # Install another version of node that makes PySvelte work way faster\n",
    "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: notebook_connected\n"
     ]
    }
   ],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
    "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
    "pio.templates['plotly'].layout.title.font.size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-20a9fb21-e005\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.34.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-20a9fb21-e005\",\n",
       "      Hello,\n",
       "      {\"name\": \"Dashiell\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f05bc25b010>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Dashiell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 53\n",
    "frac_train = 0.5\n",
    "\n",
    "# Optimizer config\n",
    "lr = 1e-3\n",
    "wd = 1. \n",
    "betas = (0.9, 0.98)\n",
    "\n",
    "num_epochs = 50000\n",
    "checkpoint_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7890481"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "53 ** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Task\n",
    "* Define modular addition\n",
    "* Define the dataset & labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input format:\n",
    "|a|b|=|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For p**4 elements add a d vector with l -> (i j k l)\n",
    "a_vector = einops.repeat(torch.arange(p), \"i -> (i j k l)\", j=p, k=p, l=p)\n",
    "b_vector = einops.repeat(torch.arange(p), \"j -> (i j k l)\", i=p, k=p, l=p)\n",
    "c_vector = einops.repeat(torch.arange(p), \"k -> (i j k l)\", i=p, j=p, l=p)\n",
    "d_vector = einops.repeat(torch.arange(p), \"l -> (i j k l)\", i=p, j=p, k=p) \n",
    "equals_vector = einops.repeat(torch.tensor(p), \" -> (i j k l)\", i=p, j=p, k=p, l=p)\n",
    "star_vector = einops.repeat(torch.tensor(p+1), \" -> (i j k l)\", i=p, j=p, k=p, l=p)\n",
    "plus_vector = einops.repeat(torch.tensor(p+2), \" -> (i j k l)\", i=p, j=p, k=p, l=p)\n",
    "caret_vector = einops.repeat(torch.tensor(p+3), \" -> (i j k l)\", i=p, j=p, k=p, l=p)\n",
    "lparen = einops.repeat(torch.tensor(p+4), \" -> (i j k l)\", i=p, j=p, k=p, l=p)\n",
    "rparen = einops.repeat(torch.tensor(p+5), \" -> (i j k l)\", i=p, j=p, k=p, l=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[57,  0, 54,  0, 55,  0, 58, 56,  0, 53],\n",
      "        [57,  0, 54,  0, 55,  0, 58, 56,  1, 53],\n",
      "        [57,  0, 54,  0, 55,  0, 58, 56,  2, 53],\n",
      "        [57,  0, 54,  0, 55,  0, 58, 56,  3, 53],\n",
      "        [57,  0, 54,  0, 55,  0, 58, 56,  4, 53]], device='cuda:0')\n",
      "torch.Size([7890481, 10])\n"
     ]
    }
   ],
   "source": [
    "dataset = torch.stack([\n",
    "    lparen,\n",
    "    a_vector,\n",
    "    star_vector,\n",
    "    b_vector,\n",
    "    plus_vector,\n",
    "    c_vector,\n",
    "    rparen,\n",
    "    caret_vector,\n",
    "    d_vector,\n",
    "    equals_vector], dim=1).cuda()\n",
    "print(dataset[:5])\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7890481])\n",
      "tensor([1, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "labels = ((dataset[:, 1] * dataset[:, 3] + dataset[:, 5]) ** dataset[:, 8]) % p\n",
    "print(labels.shape)\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this to a train + test set - 30% in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([57,  1, 54,  6, 55, 16, 58, 56,  9, 53], device='cuda:0'), tensor(34, device='cuda:0'))\n",
      "(tensor([57, 39, 54,  3, 55, 25, 58, 56, 31, 53], device='cuda:0'), tensor(29, device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(314159)\n",
    "indices = torch.randperm(p**4)\n",
    "cutoff = int((p**4)*frac_train)\n",
    "train_indices = indices[:cutoff]\n",
    "test_indices = indices[cutoff:]\n",
    "\n",
    "train_data = TensorDataset(dataset[train_indices], labels[train_indices])\n",
    "test_data = TensorDataset(dataset[test_indices], labels[test_indices])\n",
    "print(train_data[5])\n",
    "#print(train_labels[:5])\n",
    "#print(train_data.shape)\n",
    "print(test_data[5])\n",
    "#print(test_labels[:5])\n",
    "#print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 4,\n",
    "    d_model = 256,\n",
    "    d_head = 64,\n",
    "    d_mlp = 1024,\n",
    "    act_fn = \"relu\",\n",
    "    normalization_type=None,\n",
    "    d_vocab=p+6,\n",
    "    d_vocab_out=p,\n",
    "    n_ctx=10,\n",
    "    init_weights=True,\n",
    "    device=\"cuda\",\n",
    "    seed = 999,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable the biases, as we don't need them for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"b_\" in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer + Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    if len(logits.shape)==3:\n",
    "        logits = logits[:, -1]\n",
    "    logits = logits.to(torch.float64)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
    "    return -correct_log_probs.mean()\n",
    "\n",
    "#train_logits = model(train_data)\n",
    "#train_loss = loss_fn(train_logits, train_labels)\n",
    "#print(train_loss)\n",
    "#test_logits = model(test_data)\n",
    "#test_loss = loss_fn(test_logits, test_labels)\n",
    "#print(test_loss)\n",
    "\n",
    "def train_forward(model, dataloader):\n",
    "    total_loss = torch.tensor(0., device='cuda', requires_grad=False)\n",
    "    for batch, labels in dataloader:\n",
    "        logits = model(batch)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        total_loss += loss\n",
    "    return total_loss\n",
    "\n",
    "def test_forward(model, dataloader):\n",
    "    total_loss = torch.tensor(0., device='cuda', requires_grad=False)\n",
    "    for batch, labels in dataloader:\n",
    "        logits = model(batch)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform loss:\n",
      "3.970291913552122\n"
     ]
    }
   ],
   "source": [
    "print(\"Uniform loss:\")\n",
    "print(np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2 ** 16\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f5472140fd43d8b9b3f26d5d5f0380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 242.78997802734375 Test Loss 237.64923095703125\n",
      "Epoch 100 Train Loss 217.28501892089844 Test Loss 217.24752807617188\n",
      "Epoch 200 Train Loss 209.68406677246094 Test Loss 209.5689697265625\n",
      "Epoch 300 Train Loss 207.1466064453125 Test Loss 206.0689239501953\n",
      "Epoch 400 Train Loss 204.72836303710938 Test Loss 204.74916076660156\n",
      "Epoch 500 Train Loss 203.6313934326172 Test Loss 203.71217346191406\n",
      "Epoch 600 Train Loss 202.8385772705078 Test Loss 202.80880737304688\n",
      "Epoch 700 Train Loss 202.04086303710938 Test Loss 201.99293518066406\n",
      "Epoch 800 Train Loss 201.083251953125 Test Loss 201.0811767578125\n",
      "Epoch 900 Train Loss 200.2921142578125 Test Loss 200.03111267089844\n",
      "Epoch 1000 Train Loss 199.1699676513672 Test Loss 199.12059020996094\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "model_checkpoints = []\n",
    "checkpoint_epochs = []\n",
    "num_epochs = 50_000\n",
    "grok_threshold = 0.01\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    train_loss = train_forward(model, train_dataloader)\n",
    "    train_losses.append(train_loss.item())\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_loss = test_forward(model, train_dataloader)\n",
    "        test_losses.append(test_loss.item())\n",
    "    \n",
    "    if (epoch % checkpoint_every) == 0:\n",
    "        checkpoint_epochs.append(epoch)\n",
    "        model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
    "        print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")\n",
    "    if test_loss.item() <= grok_threshold:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "num_epochs = 150_000\n",
    "grok_threshold = 0.01\n",
    "\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    train_logits = model(train_data)\n",
    "    train_loss = loss_fn(train_logits, train_labels)\n",
    "    train_loss.backward()\n",
    "    train_losses.append(train_loss.item())\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(test_data)\n",
    "        test_loss = loss_fn(test_logits, test_labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "    \n",
    "    if ((epoch+1)%checkpoint_every)==0:\n",
    "        checkpoint_epochs.append(epoch)\n",
    "        model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
    "        print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")\n",
    "    if test_loss.item() <= grok_threshold:\n",
    "        break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "     {\n",
    "         \"model\":model.state_dict(),\n",
    "         \"config\": model.cfg,\n",
    "         \"checkpoints\": model_checkpoints,\n",
    "         \"checkpoint_epochs\": checkpoint_epochs,\n",
    "         \"test_losses\": test_losses,\n",
    "         \"train_losses\": train_losses,\n",
    "         \"train_indices\": train_indices,\n",
    "         \"test_indices\": test_indices\n",
    "     },\n",
    "     \"checkpoints/grokking_xyz_3333.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Model Training Statistics, Check that it groks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_losses[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(\n",
    "    train_losses[::100],\n",
    "    x=np.arange(0, len(train_losses), 100),\n",
    "    xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, \n",
    "    title=\"Training Curve for Modular Addition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Things to Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_logits, cache = model.run_with_cache(dataset)\n",
    "print(original_logits.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get key weight matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.embed.W_E[:-1]\n",
    "print(\"W_E\", W_E.shape)\n",
    "W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in\n",
    "print(\"W_neur\", W_neur.shape)\n",
    "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
    "print(\"W_logit\", W_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_loss = loss_fn(original_logits, labels).item()\n",
    "print(\"Original Loss:\", original_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_a = cache[\"pattern\", 0, \"attn\"][:, :, -1, 0]\n",
    "pattern_b = cache[\"pattern\", 0, \"attn\"][:, :, -1, 1]\n",
    "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_pre_acts = cache[\"pre\", 0, \"mlp\"][:, -1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name, param in cache.items():\n",
    "    print(param_name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(cache[\"pattern\", 0].mean(dim=0)[:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(cache[\"pattern\", 0][5][:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(cache[\"pattern\", 0][:, 0, -1, 0].reshape(p, p, p), title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(\n",
    "    einops.rearrange(cache[\"pattern\", 0][:, :, -1, 0], \"(a b) head -> head a b\", a=p, b=p), \n",
    "    title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting neuron activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[\"post\", 0, \"mlp\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(\n",
    "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p), \n",
    "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = torch.svd(W_E)\n",
    "npx.line(S, title=\"Singular Values\")\n",
    "npx.imshow(U, title=\"Principal Components on the Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control - random Gaussian matrix\n",
    "U, S, Vh = torch.svd(torch.randn_like(W_E))\n",
    "npx.line(S, title=\"Singular Values Random\")\n",
    "npx.imshow(U, title=\"Principal Components Random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Embedding - It's a Lookup Table!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = torch.svd(W_E)\n",
    "npx.line(U[:, :8].T, title=\"Principal Components of the embedding\", xaxis=\"Input Vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_basis = []\n",
    "fourier_basis_names = []\n",
    "fourier_basis.append(torch.ones(p))\n",
    "fourier_basis_names.append(\"Constant\")\n",
    "for freq in range(1, p//2+1):\n",
    "    fourier_basis.append(torch.sin(torch.arange(p)*2 * torch.pi * freq / p))\n",
    "    fourier_basis_names.append(f\"Sin {freq}\")\n",
    "    fourier_basis.append(torch.cos(torch.arange(p)*2 * torch.pi * freq / p))\n",
    "    fourier_basis_names.append(f\"Cos {freq}\")\n",
    "fourier_basis = torch.stack(fourier_basis, dim=0).cuda()\n",
    "fourier_basis = fourier_basis/fourier_basis.norm(dim=-1, keepdim=True)\n",
    "npx.imshow(fourier_basis, xaxis=\"Input\", yaxis=\"Component\", y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line(fourier_basis[:8], xaxis=\"Input\", line_labels=fourier_basis_names[:8], title=\"First 8 Fourier Components\")\n",
    "npx.line(fourier_basis[25:29], xaxis=\"Input\", line_labels=fourier_basis_names[25:29], title=\"Middle Fourier Components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ fourier_basis.T, title=\"All Fourier Vectors are Orthogonal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ W_E, yaxis=\"Fourier Component\", xaxis=\"Residual Stream\", y=fourier_basis_names, title=\"Embedding in Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line((fourier_basis @ W_E).norm(dim=-1), xaxis=\"Fourier Component\", x=fourier_basis_names, title=\"Norms of Embedding in Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_freqs = [17, 25, 32, 47]\n",
    "key_freq_indices = [33, 34, 49, 50, 63, 64, 93, 94]\n",
    "fourier_embed = fourier_basis @ W_E\n",
    "key_fourier_embed = fourier_embed[key_freq_indices]\n",
    "print(\"key_fourier_embed\", key_fourier_embed.shape)\n",
    "npx.imshow(key_fourier_embed @ key_fourier_embed.T, title=\"Dot Product of embedding of key Fourier Terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line(fourier_basis[[34, 50, 64, 94]], title=\"Cos of key freqs\", line_labels=[34, 50, 64, 94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line(fourier_basis[[34, 50, 64, 94]].mean(0), title=\"Constructive Interference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(\n",
    "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p), \n",
    "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(\n",
    "    einops.rearrange(neuron_acts[:, 0], \"(a b) -> a b\", a=p, b=p), \n",
    "    title=\"First neuron act\", xaxis=\"b\", yaxis=\"a\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis[94][None, :] * fourier_basis[94][:, None], title=\"Cos 47a * cos 47b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis[94][None, :] * fourier_basis[0][:, None], title=\"Cos 47a * const\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ neuron_acts[:, 0].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 0\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ neuron_acts[:, 5].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 5\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ torch.randn_like(neuron_acts[:, 0]).reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of RANDOM\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_neuron_acts = fourier_basis @ einops.rearrange(neuron_acts, \"(a b) neuron -> neuron a b\", a=p, b=p) @ fourier_basis.T\n",
    "# Center these by removing the mean - doesn't matter!\n",
    "fourier_neuron_acts[:, 0, 0] = 0.\n",
    "print(\"fourier_neuron_acts\", fourier_neuron_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_freq_norm = torch.zeros(p//2, model.cfg.d_mlp).cuda()\n",
    "for freq in range(0, p//2):\n",
    "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "            neuron_freq_norm[freq] += fourier_neuron_acts[:, x, y]**2\n",
    "neuron_freq_norm = neuron_freq_norm / fourier_neuron_acts.pow(2).sum(dim=[-1, -2])[None, :]\n",
    "npx.imshow(neuron_freq_norm, xaxis=\"Neuron\", yaxis=\"Freq\", y=torch.arange(1, p//2+1), title=\"Neuron Frac Explained by Freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line(neuron_freq_norm.max(dim=0).values.sort().values, xaxis=\"Neuron\", title=\"Max Neuron Frac Explained over Freqs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Off the Neuron-Logit Weights to Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
    "print(\"W_logit\", W_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line((W_logit @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit in the Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_17 = neuron_freq_norm[17-1]>0.85\n",
    "neurons_17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_17.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line((W_logit[neurons_17] @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit for freq 17 neurons in the Fourier Basis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study sin 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 17\n",
    "W_logit_fourier = W_logit @ fourier_basis\n",
    "neurons_sin_17 = W_logit_fourier[:, 2*freq-1]\n",
    "npx.line(neurons_sin_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_sin_17c = neuron_acts @ neurons_sin_17\n",
    "npx.imshow(fourier_basis @ inputs_sin_17c.reshape(p, p) @ fourier_basis.T, title=\"Fourier Heatmap over inputs for sin17c\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Box Methods + Progress Measures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
