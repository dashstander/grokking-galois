{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6913bc8b-6a29-4ad8-b1f2-68732d62d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functools import *\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# import comet_ml\n",
    "import itertools\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50c5be68-66e4-4459-9fc4-166300eb859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper class to get access to intermediate activations (inspired by Garcon)\n",
    "# It's a dummy module that is the identity function by default\n",
    "# I can wrap any intermediate activation in a HookPoint and get a convenient \n",
    "# way to add PyTorch hooks\n",
    "\n",
    "# Copied shamelessly from Neel Nanda\n",
    "\n",
    "class HookPoint(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fwd_hooks = []\n",
    "        self.bwd_hooks = []\n",
    "    \n",
    "    def give_name(self, name):\n",
    "        # Called by the model at initialisation\n",
    "        self.name = name\n",
    "    \n",
    "    def add_hook(self, hook, dir='fwd'):\n",
    "        # Hook format is fn(activation, hook_name)\n",
    "        # Change it into PyTorch hook format (this includes input and output, \n",
    "        # which are the same for a HookPoint)\n",
    "        def full_hook(module, module_input, module_output):\n",
    "            return hook(module_output, name=self.name)\n",
    "        if dir=='fwd':\n",
    "            handle = self.register_forward_hook(full_hook)\n",
    "            self.fwd_hooks.append(handle)\n",
    "        elif dir=='bwd':\n",
    "            handle = self.register_backward_hook(full_hook)\n",
    "            self.bwd_hooks.append(handle)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def remove_hooks(self, dir='fwd'):\n",
    "        if (dir=='fwd') or (dir=='both'):\n",
    "            for hook in self.fwd_hooks:\n",
    "                hook.remove()\n",
    "            self.fwd_hooks = []\n",
    "        if (dir=='bwd') or (dir=='both'):\n",
    "            for hook in self.bwd_hooks:\n",
    "                hook.remove()\n",
    "            self.bwd_hooks = []\n",
    "        if dir not in ['fwd', 'bwd', 'both']:\n",
    "            raise ValueError(f\"Invalid direction {dir}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb4890a-0e62-46df-988a-e0e11921f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define network architecture\n",
    "# I defined my own transformer from scratch so I'd fully understand each component \n",
    "# - I expect this wasn't necessary or particularly important, and a bunch of this \n",
    "# replicates existing PyTorch functionality\n",
    "\n",
    "# Copied shamelessly from Neel Nanda\n",
    "\n",
    "# Embed & Unembed\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_E = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.einsum('dbp -> bpd', self.W_E[:, x])\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, d_vocab, d_model):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.randn(d_model, d_vocab)/np.sqrt(d_vocab))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return (x @ self.W_U)\n",
    "\n",
    "# Positional Embeddings\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, max_ctx, d_model):\n",
    "        super().__init__()\n",
    "        self.W_pos = nn.Parameter(torch.randn(max_ctx, d_model)/np.sqrt(d_model))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x+self.W_pos[:x.shape[-2]]\n",
    "\n",
    "# LayerNorm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, epsilon = 1e-4, model=[None]):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.w_ln = nn.Parameter(torch.ones(d_model))\n",
    "        self.b_ln = nn.Parameter(torch.zeros(d_model))\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.model[0].use_ln:\n",
    "            x = x - x.mean(axis=-1)[..., None]\n",
    "            x = x / (x.std(axis=-1)[..., None] + self.epsilon)\n",
    "            x = x * self.w_ln\n",
    "            x = x + self.b_ln\n",
    "            return x\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_head, n_ctx, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_K = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_Q = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_V = nn.Parameter(torch.randn(num_heads, d_head, d_model)/np.sqrt(d_model))\n",
    "        self.W_O = nn.Parameter(torch.randn(d_model, d_head * num_heads)/np.sqrt(d_model))\n",
    "        self.register_buffer('mask', torch.tril(torch.ones((n_ctx, n_ctx))))\n",
    "        self.d_head = d_head\n",
    "        self.hook_k = HookPoint()\n",
    "        self.hook_q = HookPoint()\n",
    "        self.hook_v = HookPoint()\n",
    "        self.hook_z = HookPoint()\n",
    "        self.hook_attn = HookPoint()\n",
    "        self.hook_attn_pre = HookPoint()\n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.hook_k(torch.einsum('ihd,bpd->biph', self.W_K, x))\n",
    "        q = self.hook_q(torch.einsum('ihd,bpd->biph', self.W_Q, x))\n",
    "        v = self.hook_v(torch.einsum('ihd,bpd->biph', self.W_V, x))\n",
    "        attn_scores_pre = torch.einsum('biph,biqh->biqp', k, q)\n",
    "        attn_scores_masked = torch.tril(attn_scores_pre) - 1e10 * (1 - self.mask[:x.shape[-2], :x.shape[-2]])\n",
    "        attn_matrix = self.hook_attn(F.softmax(self.hook_attn_pre(attn_scores_masked/np.sqrt(self.d_head)), dim=-1))\n",
    "        z = self.hook_z(torch.einsum('biph,biqp->biqh', v, attn_matrix))\n",
    "        z_flat = einops.rearrange(z, 'b i q h -> b q (i h)')\n",
    "        out = torch.einsum('df,bqf->bqd', self.W_O, z_flat)\n",
    "        return out\n",
    "\n",
    "# MLP Layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.W_in = nn.Parameter(torch.randn(d_mlp, d_model)/np.sqrt(d_model))\n",
    "        self.b_in = nn.Parameter(torch.zeros(d_mlp))\n",
    "        self.W_out = nn.Parameter(torch.randn(d_model, d_mlp)/np.sqrt(d_model))\n",
    "        self.b_out = nn.Parameter(torch.zeros(d_model))\n",
    "        self.act_type = act_type\n",
    "        # self.ln = LayerNorm(d_mlp, model=self.model)\n",
    "        self.hook_pre = HookPoint()\n",
    "        self.hook_post = HookPoint()\n",
    "        assert act_type in ['ReLU', 'GeLU']\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hook_pre(torch.einsum('md,bpd->bpm', self.W_in, x) + self.b_in)\n",
    "        if self.act_type=='ReLU':\n",
    "            x = F.relu(x)\n",
    "        elif self.act_type=='GeLU':\n",
    "            x = F.gelu(x)\n",
    "        x = self.hook_post(x)\n",
    "        x = torch.einsum('dm,bpm->bpd', self.W_out, x) + self.b_out\n",
    "        return x\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        # self.ln1 = LayerNorm(d_model, model=self.model)\n",
    "        self.attn = Attention(d_model, num_heads, d_head, n_ctx, model=self.model)\n",
    "        # self.ln2 = LayerNorm(d_model, model=self.model)\n",
    "        self.mlp = MLP(d_model, d_mlp, act_type, model=self.model)\n",
    "        self.hook_attn_out = HookPoint()\n",
    "        self.hook_mlp_out = HookPoint()\n",
    "        self.hook_resid_pre = HookPoint()\n",
    "        self.hook_resid_mid = HookPoint()\n",
    "        self.hook_resid_post = HookPoint()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hook_resid_mid(x + self.hook_attn_out(self.attn((self.hook_resid_pre(x)))))\n",
    "        x = self.hook_resid_post(x + self.hook_mlp_out(self.mlp((x))))\n",
    "        return x\n",
    "\n",
    "# Full transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_vocab, d_model, d_mlp, d_head, num_heads, n_ctx, act_type, use_cache=False, use_ln=True):\n",
    "        super().__init__()\n",
    "        self.cache = {}\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "        self.embed = Embed(d_vocab, d_model)\n",
    "        self.pos_embed = PosEmbed(n_ctx, d_model)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, d_mlp, d_head, num_heads, n_ctx, act_type, model=[self]) for i in range(num_layers)])\n",
    "        # self.ln = LayerNorm(d_model, model=[self])\n",
    "        self.unembed = Unembed(d_vocab, d_model)\n",
    "        self.use_ln = use_ln\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if type(module)==HookPoint:\n",
    "                module.give_name(name)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.pos_embed(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        # x = self.ln(x)\n",
    "        x = self.unembed(x)\n",
    "        return x\n",
    "\n",
    "    def set_use_cache(self, use_cache):\n",
    "        self.use_cache = use_cache\n",
    "    \n",
    "    def hook_points(self):\n",
    "        return [module for name, module in self.named_modules() if 'hook' in name]\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        for hp in self.hook_points():\n",
    "            hp.remove_hooks('fwd')\n",
    "            hp.remove_hooks('bwd')\n",
    "    \n",
    "    def cache_all(self, cache, incl_bwd=False):\n",
    "        # Caches all activations wrapped in a HookPoint\n",
    "        def save_hook(tensor, name):\n",
    "            cache[name] = tensor.detach()\n",
    "        def save_hook_back(tensor, name):\n",
    "            cache[name+'_grad'] = tensor[0].detach()\n",
    "        for hp in self.hook_points():\n",
    "            hp.add_hook(save_hook, 'fwd')\n",
    "            if incl_bwd:\n",
    "                hp.add_hook(save_hook_back, 'bwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8bda4fe3-d38b-470c-b46b-9d7ff65b2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3\n",
    "weight_decay = 1.0\n",
    "p=113 \n",
    "d_model = 128 \n",
    "fn_name = 'x*y+z'\n",
    "frac_train = 0.3 \n",
    "num_epochs = 50000\n",
    "save_models = True\n",
    "save_every = 100\n",
    "# Stop training when test loss is <stopping_thresh\n",
    "stopping_thresh = -1\n",
    "seed = 0 \n",
    "\n",
    "num_layers = 1\n",
    "batch_style = 'full'\n",
    "d_vocab = p+3\n",
    "n_ctx = 6\n",
    "d_mlp = 4*d_model\n",
    "num_heads = 4\n",
    "assert d_model % num_heads == 0\n",
    "d_head = d_model//num_heads\n",
    "act_type = 'ReLU' \n",
    "batch_size = 512\n",
    "use_ln = False\n",
    "#random_answers = np.random.randint(low=0, high=p, size=(p, p))\n",
    "fns_dict = {\n",
    "    'add': lambda x,y:(x+y)%p,\n",
    "    'subtract': lambda x,y:(x-y)%p,\n",
    "    'x2xyy2':lambda x,y:(x**2+x*y+y**2)%p,\n",
    "    'rand':lambda x,y:random_answers[x][y],\n",
    "    'x*y+z': lambda x, y, z: ((x*y)+z)%p\n",
    "    \n",
    "}\n",
    "fn = fns_dict[fn_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ba7e2d3-0d00-435f-aa2f-d45071ae7623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "num = 113\n",
    "\n",
    "def gen_train_test(frac_train, num, seed=0):\n",
    "    # Generate train and test split\n",
    "    triples = [(i, j, k, num) for i in range(num) for j in range(num) for k in range(num)]\n",
    "    all_data = pl.Series('triple', triples).shuffle(seed=0)\n",
    "    div = int(frac_train*len(triples))\n",
    "    train = all_data.head(div).to_list()\n",
    "    test = all_data.tail(len(triples) - div).to_list()\n",
    "    return train, test\n",
    "    \n",
    "\n",
    "train, test = gen_train_test(0.33333, 113)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8f8249a-5367-47c7-b24b-a3e335579b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 6, 10, 113)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[5]\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97bf5364-27c4-4b2f-b29c-33a597d96357",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa1dfbbc-d3cb-4a3c-968b-040bf9ab12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def cuda_memory():\n",
    "    print(torch.cuda.memory_allocated()/1e9)\n",
    "\n",
    "def cross_entropy_high_precision(logits, labels):\n",
    "    # Shapes: batch x vocab, batch\n",
    "    # Cast logits to float64 because log_softmax has a float32 underflow on overly \n",
    "    # confident data and can only return multiples of 1.2e-7 (the smallest float x\n",
    "    # such that 1+x is different from 1 in float32). This leads to loss spikes \n",
    "    # and dodgy gradients\n",
    "    print(logits.shape)\n",
    "    print(labels.shape)\n",
    "    logprobs = F.log_softmax(logits.to(torch.float64), dim=-1)\n",
    "    prediction_logprobs = torch.gather(logprobs, index=labels[:, None], dim=-1)\n",
    "    loss = -torch.mean(prediction_logprobs)\n",
    "    return loss\n",
    "\n",
    "def full_loss(model, data):\n",
    "    # Take the final position only\n",
    "    logits = model(data)[:, -1]\n",
    "    labels = torch.tensor([fn(i, j, k) for i, j, k, _ in data])\n",
    "    return cross_entropy_high_precision(logits, labels)\n",
    "\n",
    "def test_logits(logits, bias_correction=False, original_logits=None, mode='all'):\n",
    "    # Calculates cross entropy loss of logits representing a batch of all p^2 \n",
    "    # possible inputs\n",
    "    # Batch dimension is assumed to be first\n",
    "    if logits.shape[1]==p*p:\n",
    "        logits = logits.T\n",
    "    if logits.shape==torch.Size([p*p, p+1]):\n",
    "        logits = logits[:, :-1]\n",
    "    logits = logits.reshape(p*p, p)\n",
    "    if bias_correction:\n",
    "        # Applies bias correction - we correct for any missing bias terms, \n",
    "        # independent of the input, by centering the new logits along the batch \n",
    "        # dimension, and then adding the average original logits across all inputs\n",
    "        logits = einops.reduce(original_logits - logits, 'batch ... -> ...', 'mean') + logits\n",
    "    if mode=='train':\n",
    "        return cross_entropy_high_precision(logits[is_train], labels[is_train])\n",
    "    elif mode=='test':\n",
    "        return cross_entropy_high_precision(logits[is_test], labels[is_test])\n",
    "    elif mode=='all':\n",
    "        return cross_entropy_high_precision(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c9828-cf2d-4ca1-96fc-a563105a7974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name grok_1671317095\n",
      "torch.Size([480960, 116])\n",
      "torch.Size([480960])\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_vocab=d_vocab,\n",
    "    d_model=d_model,\n",
    "    d_mlp=d_mlp,\n",
    "    d_head=d_head,\n",
    "    num_heads=num_heads,\n",
    "    n_ctx=n_ctx,\n",
    "    act_type=act_type,\n",
    "    use_cache=False,\n",
    "    use_ln=use_ln\n",
    ")\n",
    "#model.to('cuda')\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.98))\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))\n",
    "run_name = f\"grok_{int(time.time())}\"\n",
    "print(f'Run name {run_name}')\n",
    "if save_models:\n",
    "    os.mkdir(root/run_name)\n",
    "    save_dict = {'model':model.state_dict(), 'train_data':train, 'test_data':test}\n",
    "    torch.save(save_dict, root/run_name/'init.pth')\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = full_loss(model, train)\n",
    "    test_loss = full_loss(model, test)\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    if epoch%100 == 0: print(f\"{epoch}_{np.log(train_loss.item()):.4f}_{np.log(test_loss.item()):.4f}\")#_{train_acc.item():.4f}_{test_acc.item():.4f}\")\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    if test_loss.item() < stopping_thresh:\n",
    "        break\n",
    "    if (save_models) and (epoch%save_every == 0):\n",
    "        if test_loss.item() < stopping_thresh:\n",
    "            break\n",
    "        save_dict = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(save_dict, root/run_name/f\"{epoch}.pth\")\n",
    "        print(f\"Saved model to {root/run_name/f'{epoch}.pth'}\")\n",
    "if not save_models:\n",
    "    os.mkdir(root/run_name)\n",
    "save_dict = {\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'scheduler': scheduler.state_dict(),\n",
    "    'train_loss': train_loss,\n",
    "    'test_loss': test_loss,\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'epoch': epoch,\n",
    "}\n",
    "torch.save(save_dict, root/run_name/f\"final.pth\")\n",
    "print(f\"Saved model to {root/run_name/f'final.pth'}\")\n",
    "lines([train_losses, test_losses], labels=['train', 'test'], log_y=True)\n",
    "\n",
    "    # save_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd641814-7140-4609-80f3-ba5c11d4e8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
