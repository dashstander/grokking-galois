{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grokking Demo Notebook\n",
    "\n",
    "<b style=\"color: red\">To use this notebook, go to Runtime > Change Runtime Type and select GPU as the hardware accelerator.</b>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "(No need to read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "DEVELOPMENT_MODE = True\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git@new-demo\n",
    "    %pip install circuitsvis\n",
    "    \n",
    "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
    "    # # Install another version of node that makes PySvelte work way faster\n",
    "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
    "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
    "pio.templates['plotly'].layout.title.font.size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"Neel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtyping import TensorType as TT\n",
    "from typing import List, Union, Optional\n",
    "from functools import partial\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 113\n",
    "frac_train = 0.3\n",
    "\n",
    "# Optimizer config\n",
    "lr = 1e-3\n",
    "wd = 1. \n",
    "betas = (0.9, 0.98)\n",
    "\n",
    "num_epochs = 25000\n",
    "checkpoint_every = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Task\n",
    "* Define modular addition\n",
    "* Define the dataset & labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input format:\n",
    "|a|b|=|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_vector = einops.repeat(torch.arange(p), \"i -> (i j)\", j=p)\n",
    "b_vector = einops.repeat(torch.arange(p), \"j -> (i j)\", i=p)\n",
    "equals_vector = einops.repeat(torch.tensor(113), \" -> (i j)\", i=p, j=p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).cuda()\n",
    "print(dataset[:5])\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (dataset[:, 0] + dataset[:, 1]) % p\n",
    "print(labels.shape)\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this to a train + test set - 30% in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randperm(p*p)\n",
    "cutoff = int(p*p*frac_train)\n",
    "train_indices = indices[:cutoff]\n",
    "test_indices = indices[cutoff:]\n",
    "\n",
    "train_data = dataset[train_indices]\n",
    "train_labels = labels[train_indices]\n",
    "test_data = dataset[test_indices]\n",
    "test_labels = labels[test_indices]\n",
    "print(train_data[:5])\n",
    "print(train_labels[:5])\n",
    "print(train_data.shape)\n",
    "print(test_data[:5])\n",
    "print(test_labels[:5])\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 1,\n",
    "    n_heads = 4,\n",
    "    d_model = 128,\n",
    "    d_head = 32,\n",
    "    d_mlp = 512,\n",
    "    act_fn = \"relu\",\n",
    "    normalization_type=None,\n",
    "    d_vocab=p+1,\n",
    "    d_vocab_out=p,\n",
    "    n_ctx=3,\n",
    "    init_weights=True,\n",
    "    device=\"cuda\",\n",
    "    seed = 999,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable the biases, as we don't need them for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"b_\" in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Optimizer + Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, labels):\n",
    "    if len(logits.shape)==3:\n",
    "        logits = logits[:, -1]\n",
    "    logits = logits.to(torch.float64)\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
    "    return -correct_log_probs.mean()\n",
    "train_logits = model(train_data)\n",
    "train_loss = loss_fn(train_logits, train_labels)\n",
    "print(train_loss)\n",
    "test_logits = model(test_data)\n",
    "test_loss = loss_fn(test_logits, test_labels)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Uniform loss:\")\n",
    "print(np.log(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "model_checkpoints = []\n",
    "checkpoint_epochs = []\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    train_logits = model(train_data)\n",
    "    train_loss = loss_fn(train_logits, train_labels)\n",
    "    train_loss.backward()\n",
    "    train_losses.append(train_loss.item())\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model(test_data)\n",
    "        test_loss = loss_fn(test_logits, test_labels)\n",
    "        test_losses.append(test_loss.item())\n",
    "    \n",
    "    if ((epoch+1)%checkpoint_every)==0:\n",
    "        checkpoint_epochs.append(epoch)\n",
    "        model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
    "        print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(\n",
    "#     {\n",
    "#         \"model\":model.state_dict(),\n",
    "#         \"config\": model.cfg,\n",
    "#         \"checkpoints\": model_checkpoints,\n",
    "#         \"checkpoint_epochs\": checkpoint_epochs,\n",
    "#         \"test_losses\": test_losses,\n",
    "#         \"train_losses\": train_losses,\n",
    "#     },\n",
    "#     \"/workspace/_scratch/grokking_demo.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Model Training Statistics, Check that it groks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neel.plot as npx\n",
    "npx.line([train_losses[::100], test_losses[::100]], x=np.arange(0, len(train_losses), 100), xaxis=\"Epoch\", yaxis=\"Loss\", log_y=True, title=\"Training Curve for Modular Addition\", line_labels=['train', 'test'], toggle_x=True, toggle_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Things to Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_logits, cache = model.run_with_cache(dataset)\n",
    "print(original_logits.numel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get key weight matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E = model.embed.W_E[:-1]\n",
    "print(\"W_E\", W_E.shape)\n",
    "W_neur = W_E @ model.blocks[0].attn.W_V @ model.blocks[0].attn.W_O @ model.blocks[0].mlp.W_in\n",
    "print(\"W_neur\", W_neur.shape)\n",
    "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
    "print(\"W_logit\", W_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_loss = loss_fn(original_logits, labels).item()\n",
    "print(\"Original Loss:\", original_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Activations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_a = cache[\"pattern\", 0, \"attn\"][:, :, -1, 0]\n",
    "pattern_b = cache[\"pattern\", 0, \"attn\"][:, :, -1, 1]\n",
    "neuron_acts = cache[\"post\", 0, \"mlp\"][:, -1, :]\n",
    "neuron_pre_acts = cache[\"pre\", 0, \"mlp\"][:, -1, :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_name, param in cache.items():\n",
    "    print(param_name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(cache[\"pattern\", 0].mean(dim=0)[:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(cache[\"pattern\", 0][5][:, -1, :], title=\"Average Attention Pattern per Head\", xaxis=\"Source\", yaxis=\"Head\", x=['a', 'b', '='])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(cache[\"pattern\", 0][:, 0, -1, 0].reshape(p, p), title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(\n",
    "    einops.rearrange(cache[\"pattern\", 0][:, :, -1, 0], \"(a b) head -> head a b\", a=p, b=p), \n",
    "    title=\"Attention for Head 0 from a -> =\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting neuron activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[\"post\", 0, \"mlp\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(\n",
    "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p), \n",
    "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = torch.svd(W_E)\n",
    "npx.line(S, title=\"Singular Values\")\n",
    "npx.imshow(U, title=\"Principal Components on the Input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control - random Gaussian matrix\n",
    "U, S, Vh = torch.svd(torch.randn_like(W_E))\n",
    "npx.line(S, title=\"Singular Values Random\")\n",
    "npx.imshow(U, title=\"Principal Components Random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Embedding - It's a Lookup Table!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = torch.svd(W_E)\n",
    "npx.line(U[:, :8].T, title=\"Principal Components of the embedding\", xaxis=\"Input Vocabulary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_basis = []\n",
    "fourier_basis_names = []\n",
    "fourier_basis.append(torch.ones(p))\n",
    "fourier_basis_names.append(\"Constant\")\n",
    "for freq in range(1, p//2+1):\n",
    "    fourier_basis.append(torch.sin(torch.arange(p)*2 * torch.pi * freq / p))\n",
    "    fourier_basis_names.append(f\"Sin {freq}\")\n",
    "    fourier_basis.append(torch.cos(torch.arange(p)*2 * torch.pi * freq / p))\n",
    "    fourier_basis_names.append(f\"Cos {freq}\")\n",
    "fourier_basis = torch.stack(fourier_basis, dim=0).cuda()\n",
    "fourier_basis = fourier_basis/fourier_basis.norm(dim=-1, keepdim=True)\n",
    "npx.imshow(fourier_basis, xaxis=\"Input\", yaxis=\"Component\", y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line(fourier_basis[:8], xaxis=\"Input\", line_labels=fourier_basis_names[:8], title=\"First 8 Fourier Components\")\n",
    "npx.line(fourier_basis[25:29], xaxis=\"Input\", line_labels=fourier_basis_names[25:29], title=\"Middle Fourier Components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ fourier_basis.T, title=\"All Fourier Vectors are Orthogonal\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse the Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ W_E, yaxis=\"Fourier Component\", xaxis=\"Residual Stream\", y=fourier_basis_names, title=\"Embedding in Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line((fourier_basis @ W_E).norm(dim=-1), xaxis=\"Fourier Component\", x=fourier_basis_names, title=\"Norms of Embedding in Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_freqs = [17, 25, 32, 47]\n",
    "key_freq_indices = [33, 34, 49, 50, 63, 64, 93, 94]\n",
    "fourier_embed = fourier_basis @ W_E\n",
    "key_fourier_embed = fourier_embed[key_freq_indices]\n",
    "print(\"key_fourier_embed\", key_fourier_embed.shape)\n",
    "npx.imshow(key_fourier_embed @ key_fourier_embed.T, title=\"Dot Product of embedding of key Fourier Terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line(fourier_basis[[34, 50, 64, 94]], title=\"Cos of key freqs\", line_labels=[34, 50, 64, 94])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line(fourier_basis[[34, 50, 64, 94]].mean(0), title=\"Constructive Interference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(\n",
    "    einops.rearrange(neuron_acts[:, :5], \"(a b) neuron -> neuron a b\", a=p, b=p), \n",
    "    title=\"First 5 neuron acts\", xaxis=\"b\", yaxis=\"a\", facet_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(\n",
    "    einops.rearrange(neuron_acts[:, 0], \"(a b) -> a b\", a=p, b=p), \n",
    "    title=\"First neuron act\", xaxis=\"b\", yaxis=\"a\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis[94][None, :] * fourier_basis[94][:, None], title=\"Cos 47a * cos 47b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis[94][None, :] * fourier_basis[0][:, None], title=\"Cos 47a * const\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ neuron_acts[:, 0].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 0\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ neuron_acts[:, 5].reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of neuron 5\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.imshow(fourier_basis @ torch.randn_like(neuron_acts[:, 0]).reshape(p, p) @ fourier_basis.T, title=\"2D Fourier Transformer of RANDOM\", xaxis=\"b\", yaxis=\"a\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier_neuron_acts = fourier_basis @ einops.rearrange(neuron_acts, \"(a b) neuron -> neuron a b\", a=p, b=p) @ fourier_basis.T\n",
    "# Center these by removing the mean - doesn't matter!\n",
    "fourier_neuron_acts[:, 0, 0] = 0.\n",
    "print(\"fourier_neuron_acts\", fourier_neuron_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_freq_norm = torch.zeros(p//2, model.cfg.d_mlp).cuda()\n",
    "for freq in range(0, p//2):\n",
    "    for x in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "        for y in [0, 2*(freq+1) - 1, 2*(freq+1)]:\n",
    "            neuron_freq_norm[freq] += fourier_neuron_acts[:, x, y]**2\n",
    "neuron_freq_norm = neuron_freq_norm / fourier_neuron_acts.pow(2).sum(dim=[-1, -2])[None, :]\n",
    "npx.imshow(neuron_freq_norm, xaxis=\"Neuron\", yaxis=\"Freq\", y=torch.arange(1, p//2+1), title=\"Neuron Frac Explained by Freq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line(neuron_freq_norm.max(dim=0).values.sort().values, xaxis=\"Neuron\", title=\"Max Neuron Frac Explained over Freqs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Off the Neuron-Logit Weights to Interpret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_logit = model.blocks[0].mlp.W_out @ model.unembed.W_U\n",
    "print(\"W_logit\", W_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line((W_logit @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit in the Fourier Basis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_17 = neuron_freq_norm[17-1]>0.85\n",
    "neurons_17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_17.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npx.line((W_logit[neurons_17] @ fourier_basis.T).norm(dim=0), x=fourier_basis_names, title=\"W_logit for freq 17 neurons in the Fourier Basis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study sin 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 17\n",
    "W_logit_fourier = W_logit @ fourier_basis\n",
    "neurons_sin_17 = W_logit_fourier[:, 2*freq-1]\n",
    "npx.line(neurons_sin_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_sin_17c = neuron_acts @ neurons_sin_17\n",
    "npx.imshow(fourier_basis @ inputs_sin_17c.reshape(p, p) @ fourier_basis.T, title=\"Fourier Heatmap over inputs for sin17c\", x=fourier_basis_names, y=fourier_basis_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Box Methods + Progress Measures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
